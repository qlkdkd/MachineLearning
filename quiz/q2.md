1. 선형 회귀의 경우 $J(w, b)$가 0에 매우 가까워지는 매개변수 w와 b를 찾았다면, 어떤 결론을 내릴 수 있는가?
> 1. 매개변수 w와 b의 선택된 값은 알고리즘이 훈련 세트에 실제로 잘 맞도록 만든다.
> 2. 이는 결코 가능하지 않다. 코드에 버그가 있을 것이다.
> 3. 매개변수 w와 b의 선택된 값으로 인해 알고리즘이 훈련 세트에 제대로 적합하지 않게 될 것이다.
>
> 정답: 1. 매개변수 w와 b의 선택된 값은 알고리즘이 훈련 세트에 실제로 잘 맞도록 만든다.

2. 경사하강법은 비용함수 J를 최소화하는 매개변수 w와 b의 값을 찾는 알고리즘이다.
```
repeat until convergence{
  w=w-alpha*dJ(w, b)/dw
  b=b-alpha*dJ(w, b)/db
}
```
$\partial J (w,b) \over \partial w$가 음수인 경우, 한 업데이트 단계 후에 w는 어떻게 되는가?
> 1. w는 그대로이다.
> 2. w는 증가한다.
> 3. w는 감소한다.
> 4. 알수 없다.
>
> 정답: 3. w는 감소한다.

3. 선형 회귀의 경우 매개변수 b 의 업데이트 단계는 무엇인가?
> 1. $b = b -\alpha { 1 \over m} \sum_{i=1}^m (f_{w,b} ( x^{(i)}) - y^{(i)})$
> 2. $b = b -\alpha { 1 \over m} \sum_{i=1}^m (f_{w,b} ( x^{(i)}) - y^{(i)})$
>
> 정답: 1. $b = b -\alpha { 1 \over m} \sum_{i=1}^m (f_{w,b} ( x^{(i)}) - y^{(i)})$
